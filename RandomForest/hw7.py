import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
    roc_curve,
    precision_recall_curve,
)
import matplotlib.pyplot as plt

#ë°ì´í„° í˜¸ì¶œ
df = pd.read_excel("C:/Users/user/Desktop/ìˆ˜ì—…ê´€ë ¨/3í•™ë…„ 2025/1í•™ê¸°/ì¸ê³µì§€ëŠ¥/ê³¼ì œ_ì¥ì¸í™˜/RandomForest/archive/MainData.xlsx")

# ë°ì´í„° ë¶„ë¦¬
X = df.drop("target", axis=1)  # 'target' ì¢…ì†ë³€ìˆ˜
y = df["target"]

# í›ˆë ¨ë°ì´í„° / í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ë¶„
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# ëª¨ë¸ í•™ìŠµ
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 


#print("ğŸ”¹ Confusion Matrix:")
#print(confusion_matrix(y_test, y_pred))

# ì„±ëŠ¥ ì¶œë ¥
print("\nğŸ”¹ Classification Report:")
print(classification_report(y_test, y_pred))

# ROC AUC & PR AUC ê³„ì‚°
roc_auc = roc_auc_score(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)
print(f"ğŸ”¸ ROC AUC: {roc_auc:.4f}")
print(f"ğŸ”¸ PR AUC: {pr_auc:.4f}")

# ROC Curve 
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure()
plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.4f}")
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# PR Curve 
precision, recall, _ = precision_recall_curve(y_test, y_proba)
plt.figure()
plt.plot(recall, precision, label=f"PR AUC = {pr_auc:.4f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid()
plt.show()